{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch \n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('anna.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('\"', '&', 'P', 'A', 'L', 'h', 'b', 'T', '`', 'y', 'm', '-', 'U', '_', 'a', '.', 'c', 'd', 'B', 'w', 'R', '4', '6', 'e', ':', 's', '9', 'X', 'K', 'g', 'z', 'N', '0', 'E', 'M', 'r', 'H', 'O', '1', '3', '*', '@', '$', 't', 'x', ' ', 'G', 'I', 'q', 'i', \"'\", 'C', 'p', ';', 'v', 'j', 'V', '7', 'J', 'l', 'n', '2', 'F', 'Z', '\\n', 'S', 'D', 'f', ')', ',', '8', '?', 'o', '(', 'Q', '%', 'W', '/', 'Y', '5', 'u', 'k', '!')\n"
     ]
    }
   ],
   "source": [
    "chars = tuple(set(text))\n",
    "print(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83\n"
     ]
    }
   ],
   "source": [
    "print(len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "int2char = dict(enumerate(chars))\n",
    "char2int = { ch : ii for ii,ch in int2char.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded = np.array([char2int[ch] for ch in text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1985223\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(arr, n_labels):\n",
    "    \n",
    "    # Initialize the the encoded array\n",
    "    one_hot = np.zeros((np.multiply(*arr.shape), n_labels), dtype=np.float32)\n",
    "    \n",
    "    # Fill the appropriate elements with ones\n",
    "    one_hot[np.arange(one_hot.shape[0]), arr.flatten()] = 1.\n",
    "    \n",
    "    # Finally reshape it to get back to the original array\n",
    "    one_hot = one_hot.reshape((*arr.shape, n_labels))\n",
    "    \n",
    "    return one_hot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_batches(arr, n_seqs, n_steps):\n",
    "    '''Create a generator that returns mini-batches of size\n",
    "       n_seqs x n_steps from arr.\n",
    "    '''\n",
    "    \n",
    "    batch_size = n_seqs * n_steps\n",
    "    n_batches = len(arr)//batch_size\n",
    "    \n",
    "    # Keep only enough characters to make full batches\n",
    "    arr = arr[:n_batches * batch_size]\n",
    "    # Reshape into n_seqs rows\n",
    "    arr = arr.reshape((n_seqs, -1))\n",
    "    \n",
    "    for n in range(0, arr.shape[1], n_steps):\n",
    "        # The features\n",
    "        x = arr[:, n:n+n_steps]\n",
    "        # The targets, shifted by one\n",
    "        y = np.zeros_like(x)\n",
    "        try:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, n+n_steps]\n",
    "        except IndexError:\n",
    "            y[:, :-1], y[:, -1] = x[:, 1:], arr[:, 0]\n",
    "        yield x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, tokens, n_steps=100, n_hidden=256, n_layers=2,\n",
    "                               drop_prob=0.5, lr=0.001):\n",
    "        super().__init__()\n",
    "        self.drop_prob = drop_prob\n",
    "        self.n_layers = n_layers\n",
    "        self.n_hidden = n_hidden\n",
    "        self.lr = lr\n",
    "        \n",
    "        self.chars = tokens\n",
    "        self.int2char = dict(enumerate(self.chars))\n",
    "        self.char2int = {ch: ii for ii, ch in self.int2char.items()}\n",
    "        \n",
    "        self.dropout = nn.Dropout(drop_prob)\n",
    "        self.lstm = nn.LSTM(len(self.chars), n_hidden, n_layers, \n",
    "                            dropout=drop_prob, batch_first=True)\n",
    "        self.fc = nn.Linear(n_hidden, len(self.chars))\n",
    "        \n",
    "        self.init_weights()\n",
    "        \n",
    "    def forward(self, x, hc):\n",
    "        ''' Forward pass through the network '''\n",
    "        \n",
    "        x, (h, c) = self.lstm(x, hc)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        # Stack up LSTM outputs\n",
    "        x = x.view(x.size()[0]*x.size()[1], self.n_hidden)\n",
    "        \n",
    "        x = self.fc(x)\n",
    "        \n",
    "        return x, (h, c)\n",
    "    \n",
    "    def predict(self, char, h=None, cuda=False, top_k=None):\n",
    "        ''' Given a character, predict the next character.\n",
    "        \n",
    "            Returns the predicted character and the hidden state.\n",
    "        '''\n",
    "        if cuda:\n",
    "            self.cuda()\n",
    "        else:\n",
    "            self.cpu()\n",
    "        \n",
    "        if h is None:\n",
    "            h = self.init_hidden(1)\n",
    "        \n",
    "        x = np.array([[self.char2int[char]]])\n",
    "        x = one_hot_encode(x, len(self.chars))\n",
    "        inputs = Variable(torch.from_numpy(x), volatile=True)\n",
    "        if cuda:\n",
    "            inputs = inputs.cuda()\n",
    "        \n",
    "        h = tuple([Variable(each.data, volatile=True) for each in h])\n",
    "        out, h = self.forward(inputs, h)\n",
    "\n",
    "        p = F.softmax(out).data\n",
    "        if cuda:\n",
    "            p = p.cpu()\n",
    "        \n",
    "        if top_k is None:\n",
    "            top_ch = np.arange(len(self.chars))\n",
    "        else:\n",
    "            p, top_ch = p.topk(top_k)\n",
    "            top_ch = top_ch.numpy().squeeze()\n",
    "        \n",
    "        p = p.numpy().squeeze()\n",
    "        char = np.random.choice(top_ch, p=p/p.sum())\n",
    "            \n",
    "        return self.int2char[char], h\n",
    "    \n",
    "    def init_weights(self):\n",
    "        ''' Initialize weights for fully connected layer '''\n",
    "        initrange = 0.1\n",
    "        \n",
    "        # Set bias tensor to all zeros\n",
    "        self.fc.bias.data.fill_(0)\n",
    "        # FC weights as random uniform\n",
    "        self.fc.weight.data.uniform_(-1, 1)\n",
    "        \n",
    "    def init_hidden(self, n_seqs):\n",
    "        ''' Initializes hidden state '''\n",
    "        # Create two new tensors with sizes n_layers x n_seqs x n_hidden,\n",
    "        # initialized to zero, for hidden state and cell state of LSTM\n",
    "        weight = next(self.parameters()).data\n",
    "        return (Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()),\n",
    "                Variable(weight.new(self.n_layers, n_seqs, self.n_hidden).zero_()))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(net, data, epochs=10, n_seqs=10, n_steps=50, lr=0.001, clip=5, val_frac=0.1, cuda=False, print_every=10):\n",
    "    ''' Traing a network \n",
    "    \n",
    "        Arguments\n",
    "        ---------\n",
    "        \n",
    "        net: CharRNN network\n",
    "        data: text data to train the network\n",
    "        epochs: Number of epochs to train\n",
    "        n_seqs: Number of mini-sequences per mini-batch, aka batch size\n",
    "        n_steps: Number of character steps per mini-batch\n",
    "        lr: learning rate\n",
    "        clip: gradient clipping\n",
    "        val_frac: Fraction of data to hold out for validation\n",
    "        cuda: Train with CUDA on a GPU\n",
    "        print_every: Number of steps for printing training and validation loss\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    net.train()\n",
    "    opt = torch.optim.Adam(net.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    # create training and validation data\n",
    "    val_idx = int(len(data)*(1-val_frac))\n",
    "    data, val_data = data[:val_idx], data[val_idx:]\n",
    "    \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    \n",
    "    counter = 0\n",
    "    n_chars = len(net.chars)\n",
    "    for e in range(epochs):\n",
    "        h = net.init_hidden(n_seqs)\n",
    "        for x, y in get_batches(data, n_seqs, n_steps):\n",
    "            counter += 1\n",
    "            \n",
    "            # One-hot encode our data and make them Torch tensors\n",
    "            x = one_hot_encode(x, n_chars)\n",
    "            x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "            \n",
    "            inputs, targets = Variable(x), Variable(y.long())\n",
    "            if cuda:\n",
    "                inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "            # Creating new variables for the hidden state, otherwise\n",
    "            # we'd backprop through the entire training history\n",
    "            h = tuple([Variable(each.data) for each in h])\n",
    "\n",
    "            net.zero_grad()\n",
    "            \n",
    "            output, h = net.forward(inputs, h)\n",
    "            loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "\n",
    "            loss.backward()\n",
    "            \n",
    "            # `clip_grad_norm` helps prevent the exploding gradient problem in RNNs / LSTMs.\n",
    "            nn.utils.clip_grad_norm_(net.parameters(), clip)\n",
    "\n",
    "            opt.step()\n",
    "            \n",
    "            if counter % print_every == 0:\n",
    "                \n",
    "                # Get validation loss\n",
    "                val_h = net.init_hidden(n_seqs)\n",
    "                val_losses = []\n",
    "                for x, y in get_batches(val_data, n_seqs, n_steps):\n",
    "                    # One-hot encode our data and make them Torch tensors\n",
    "                    x = one_hot_encode(x, n_chars)\n",
    "                    x, y = torch.from_numpy(x), torch.from_numpy(y)\n",
    "                    \n",
    "                    # Creating new variables for the hidden state, otherwise\n",
    "                    # we'd backprop through the entire training history\n",
    "                    val_h = tuple([Variable(each.data, volatile=True) for each in val_h])\n",
    "                    \n",
    "                    inputs, targets = Variable(x, volatile=True), Variable(y.long(), volatile=True)\n",
    "                    if cuda:\n",
    "                        inputs, targets = inputs.cuda(), targets.cuda()\n",
    "\n",
    "                    output, val_h = net.forward(inputs, val_h)\n",
    "                    val_loss = criterion(output, targets.view(n_seqs*n_steps))\n",
    "                \n",
    "                    val_losses.append(val_loss.data[0])\n",
    "                \n",
    "                print(\"Epoch: {}/{}...\".format(e+1, epochs),\n",
    "                      \"Step: {}...\".format(counter),\n",
    "                      \"Loss: {:.4f}...\".format(loss.data[0]),\n",
    "                      \"Val Loss: {:.4f}\".format(np.mean(val_losses)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CharRNN(chars, n_hidden=512, n_layers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\swapnil.walke\\appdata\\local\\continuum\\anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:74: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "c:\\users\\swapnil.walke\\appdata\\local\\continuum\\anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:76: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "c:\\users\\swapnil.walke\\appdata\\local\\continuum\\anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:83: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n",
      "c:\\users\\swapnil.walke\\appdata\\local\\continuum\\anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:87: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/25... Step: 10... Loss: 3.3152... Val Loss: 3.3121\n",
      "Epoch: 1/25... Step: 20... Loss: 3.1955... Val Loss: 3.2105\n",
      "Epoch: 1/25... Step: 30... Loss: 3.1042... Val Loss: 3.0896\n",
      "Epoch: 1/25... Step: 40... Loss: 2.9229... Val Loss: 2.9301\n",
      "Epoch: 1/25... Step: 50... Loss: 2.7912... Val Loss: 2.7533\n",
      "Epoch: 1/25... Step: 60... Loss: 2.6062... Val Loss: 2.6364\n",
      "Epoch: 1/25... Step: 70... Loss: 2.5378... Val Loss: 2.5584\n",
      "Epoch: 1/25... Step: 80... Loss: 2.4679... Val Loss: 2.5025\n",
      "Epoch: 1/25... Step: 90... Loss: 2.4489... Val Loss: 2.4611\n",
      "Epoch: 1/25... Step: 100... Loss: 2.3942... Val Loss: 2.4232\n",
      "Epoch: 1/25... Step: 110... Loss: 2.3392... Val Loss: 2.3899\n",
      "Epoch: 1/25... Step: 120... Loss: 2.2875... Val Loss: 2.3584\n",
      "Epoch: 1/25... Step: 130... Loss: 2.3015... Val Loss: 2.3379\n",
      "Epoch: 2/25... Step: 140... Loss: 2.2583... Val Loss: 2.3077\n",
      "Epoch: 2/25... Step: 150... Loss: 2.2499... Val Loss: 2.2854\n",
      "Epoch: 2/25... Step: 160... Loss: 2.2271... Val Loss: 2.2545\n",
      "Epoch: 2/25... Step: 170... Loss: 2.1818... Val Loss: 2.2335\n",
      "Epoch: 2/25... Step: 180... Loss: 2.1482... Val Loss: 2.2149\n",
      "Epoch: 2/25... Step: 190... Loss: 2.0818... Val Loss: 2.1890\n",
      "Epoch: 2/25... Step: 200... Loss: 2.0896... Val Loss: 2.1696\n",
      "Epoch: 2/25... Step: 210... Loss: 2.0903... Val Loss: 2.1489\n",
      "Epoch: 2/25... Step: 220... Loss: 2.0321... Val Loss: 2.1326\n",
      "Epoch: 2/25... Step: 230... Loss: 2.0503... Val Loss: 2.1107\n",
      "Epoch: 2/25... Step: 240... Loss: 2.0377... Val Loss: 2.0995\n",
      "Epoch: 2/25... Step: 250... Loss: 1.9787... Val Loss: 2.0765\n",
      "Epoch: 2/25... Step: 260... Loss: 1.9590... Val Loss: 2.0637\n",
      "Epoch: 2/25... Step: 270... Loss: 1.9853... Val Loss: 2.0500\n",
      "Epoch: 3/25... Step: 280... Loss: 1.9620... Val Loss: 2.0320\n",
      "Epoch: 3/25... Step: 290... Loss: 1.9585... Val Loss: 2.0367\n",
      "Epoch: 3/25... Step: 300... Loss: 1.9215... Val Loss: 2.0038\n",
      "Epoch: 3/25... Step: 310... Loss: 1.9113... Val Loss: 1.9894\n",
      "Epoch: 3/25... Step: 320... Loss: 1.8914... Val Loss: 1.9918\n",
      "Epoch: 3/25... Step: 330... Loss: 1.8768... Val Loss: 1.9790\n",
      "Epoch: 3/25... Step: 340... Loss: 1.9106... Val Loss: 1.9735\n",
      "Epoch: 3/25... Step: 350... Loss: 1.8600... Val Loss: 1.9538\n",
      "Epoch: 3/25... Step: 360... Loss: 1.8122... Val Loss: 1.9457\n",
      "Epoch: 3/25... Step: 370... Loss: 1.8442... Val Loss: 1.9332\n",
      "Epoch: 3/25... Step: 380... Loss: 1.8341... Val Loss: 1.9215\n",
      "Epoch: 3/25... Step: 390... Loss: 1.8035... Val Loss: 1.9142\n",
      "Epoch: 3/25... Step: 400... Loss: 1.7811... Val Loss: 1.9110\n",
      "Epoch: 3/25... Step: 410... Loss: 1.8010... Val Loss: 1.8981\n",
      "Epoch: 4/25... Step: 420... Loss: 1.7965... Val Loss: 1.8901\n",
      "Epoch: 4/25... Step: 430... Loss: 1.7844... Val Loss: 1.8728\n",
      "Epoch: 4/25... Step: 440... Loss: 1.7725... Val Loss: 1.8635\n",
      "Epoch: 4/25... Step: 450... Loss: 1.7257... Val Loss: 1.8558\n",
      "Epoch: 4/25... Step: 460... Loss: 1.7144... Val Loss: 1.8531\n",
      "Epoch: 4/25... Step: 470... Loss: 1.7684... Val Loss: 1.8434\n",
      "Epoch: 4/25... Step: 480... Loss: 1.7444... Val Loss: 1.8381\n",
      "Epoch: 4/25... Step: 490... Loss: 1.7457... Val Loss: 1.8268\n",
      "Epoch: 4/25... Step: 500... Loss: 1.7292... Val Loss: 1.8124\n",
      "Epoch: 4/25... Step: 510... Loss: 1.7020... Val Loss: 1.8067\n",
      "Epoch: 4/25... Step: 520... Loss: 1.7297... Val Loss: 1.8017\n",
      "Epoch: 4/25... Step: 530... Loss: 1.6932... Val Loss: 1.7985\n",
      "Epoch: 4/25... Step: 540... Loss: 1.6489... Val Loss: 1.7870\n",
      "Epoch: 4/25... Step: 550... Loss: 1.7151... Val Loss: 1.7762\n",
      "Epoch: 5/25... Step: 560... Loss: 1.6715... Val Loss: 1.7750\n",
      "Epoch: 5/25... Step: 570... Loss: 1.6689... Val Loss: 1.7635\n",
      "Epoch: 5/25... Step: 580... Loss: 1.6513... Val Loss: 1.7596\n",
      "Epoch: 5/25... Step: 590... Loss: 1.6361... Val Loss: 1.7551\n",
      "Epoch: 5/25... Step: 600... Loss: 1.6307... Val Loss: 1.7522\n",
      "Epoch: 5/25... Step: 610... Loss: 1.6141... Val Loss: 1.7402\n",
      "Epoch: 5/25... Step: 620... Loss: 1.6201... Val Loss: 1.7394\n",
      "Epoch: 5/25... Step: 630... Loss: 1.6403... Val Loss: 1.7330\n",
      "Epoch: 5/25... Step: 640... Loss: 1.6048... Val Loss: 1.7290\n",
      "Epoch: 5/25... Step: 650... Loss: 1.6168... Val Loss: 1.7216\n",
      "Epoch: 5/25... Step: 660... Loss: 1.5817... Val Loss: 1.7152\n",
      "Epoch: 5/25... Step: 670... Loss: 1.6052... Val Loss: 1.7100\n",
      "Epoch: 5/25... Step: 680... Loss: 1.6120... Val Loss: 1.7038\n",
      "Epoch: 5/25... Step: 690... Loss: 1.5925... Val Loss: 1.6993\n",
      "Epoch: 6/25... Step: 700... Loss: 1.5878... Val Loss: 1.7035\n",
      "Epoch: 6/25... Step: 710... Loss: 1.5786... Val Loss: 1.6926\n",
      "Epoch: 6/25... Step: 720... Loss: 1.5648... Val Loss: 1.6832\n",
      "Epoch: 6/25... Step: 730... Loss: 1.5733... Val Loss: 1.6835\n",
      "Epoch: 6/25... Step: 740... Loss: 1.5413... Val Loss: 1.6817\n",
      "Epoch: 6/25... Step: 750... Loss: 1.5326... Val Loss: 1.6690\n",
      "Epoch: 6/25... Step: 760... Loss: 1.5716... Val Loss: 1.6674\n",
      "Epoch: 6/25... Step: 770... Loss: 1.5496... Val Loss: 1.6664\n",
      "Epoch: 6/25... Step: 780... Loss: 1.5371... Val Loss: 1.6528\n",
      "Epoch: 6/25... Step: 790... Loss: 1.5144... Val Loss: 1.6521\n",
      "Epoch: 6/25... Step: 800... Loss: 1.5483... Val Loss: 1.6732\n",
      "Epoch: 6/25... Step: 810... Loss: 1.5301... Val Loss: 1.6494\n",
      "Epoch: 6/25... Step: 820... Loss: 1.4855... Val Loss: 1.6425\n",
      "Epoch: 6/25... Step: 830... Loss: 1.5338... Val Loss: 1.6393\n",
      "Epoch: 7/25... Step: 840... Loss: 1.4917... Val Loss: 1.6352\n",
      "Epoch: 7/25... Step: 850... Loss: 1.4997... Val Loss: 1.6339\n",
      "Epoch: 7/25... Step: 860... Loss: 1.4824... Val Loss: 1.6313\n",
      "Epoch: 7/25... Step: 870... Loss: 1.5018... Val Loss: 1.6268\n",
      "Epoch: 7/25... Step: 880... Loss: 1.5019... Val Loss: 1.6192\n",
      "Epoch: 7/25... Step: 890... Loss: 1.5038... Val Loss: 1.6225\n",
      "Epoch: 7/25... Step: 900... Loss: 1.4890... Val Loss: 1.6197\n",
      "Epoch: 7/25... Step: 910... Loss: 1.4610... Val Loss: 1.6082\n",
      "Epoch: 7/25... Step: 920... Loss: 1.4770... Val Loss: 1.6105\n",
      "Epoch: 7/25... Step: 930... Loss: 1.4739... Val Loss: 1.6106\n",
      "Epoch: 7/25... Step: 940... Loss: 1.4633... Val Loss: 1.6073\n",
      "Epoch: 7/25... Step: 950... Loss: 1.4811... Val Loss: 1.6072\n",
      "Epoch: 7/25... Step: 960... Loss: 1.4925... Val Loss: 1.5937\n",
      "Epoch: 7/25... Step: 970... Loss: 1.4839... Val Loss: 1.5964\n",
      "Epoch: 8/25... Step: 980... Loss: 1.4494... Val Loss: 1.5933\n",
      "Epoch: 8/25... Step: 990... Loss: 1.4599... Val Loss: 1.5882\n",
      "Epoch: 8/25... Step: 1000... Loss: 1.4471... Val Loss: 1.5916\n",
      "Epoch: 8/25... Step: 1010... Loss: 1.4892... Val Loss: 1.5914\n",
      "Epoch: 8/25... Step: 1020... Loss: 1.4678... Val Loss: 1.5789\n",
      "Epoch: 8/25... Step: 1030... Loss: 1.4481... Val Loss: 1.5798\n",
      "Epoch: 8/25... Step: 1040... Loss: 1.4548... Val Loss: 1.5824\n",
      "Epoch: 8/25... Step: 1050... Loss: 1.4299... Val Loss: 1.5709\n",
      "Epoch: 8/25... Step: 1060... Loss: 1.4236... Val Loss: 1.5712\n",
      "Epoch: 8/25... Step: 1070... Loss: 1.4400... Val Loss: 1.5644\n",
      "Epoch: 8/25... Step: 1080... Loss: 1.4391... Val Loss: 1.5693\n",
      "Epoch: 8/25... Step: 1090... Loss: 1.4202... Val Loss: 1.5680\n",
      "Epoch: 8/25... Step: 1100... Loss: 1.4156... Val Loss: 1.5594\n",
      "Epoch: 8/25... Step: 1110... Loss: 1.4213... Val Loss: 1.5620\n",
      "Epoch: 9/25... Step: 1120... Loss: 1.4314... Val Loss: 1.5605\n",
      "Epoch: 9/25... Step: 1130... Loss: 1.4232... Val Loss: 1.5499\n",
      "Epoch: 9/25... Step: 1140... Loss: 1.4268... Val Loss: 1.5545\n",
      "Epoch: 9/25... Step: 1150... Loss: 1.4413... Val Loss: 1.5519\n",
      "Epoch: 9/25... Step: 1160... Loss: 1.4069... Val Loss: 1.5440\n",
      "Epoch: 9/25... Step: 1170... Loss: 1.4090... Val Loss: 1.5465\n",
      "Epoch: 9/25... Step: 1180... Loss: 1.3948... Val Loss: 1.5456\n",
      "Epoch: 9/25... Step: 1190... Loss: 1.4409... Val Loss: 1.5420\n",
      "Epoch: 9/25... Step: 1200... Loss: 1.3831... Val Loss: 1.5353\n",
      "Epoch: 9/25... Step: 1210... Loss: 1.3890... Val Loss: 1.5398\n",
      "Epoch: 9/25... Step: 1220... Loss: 1.3940... Val Loss: 1.5348\n",
      "Epoch: 9/25... Step: 1230... Loss: 1.3758... Val Loss: 1.5370\n",
      "Epoch: 9/25... Step: 1240... Loss: 1.3839... Val Loss: 1.5294\n",
      "Epoch: 9/25... Step: 1250... Loss: 1.3953... Val Loss: 1.5355\n",
      "Epoch: 10/25... Step: 1260... Loss: 1.3951... Val Loss: 1.5321\n",
      "Epoch: 10/25... Step: 1270... Loss: 1.3966... Val Loss: 1.5262\n",
      "Epoch: 10/25... Step: 1280... Loss: 1.3903... Val Loss: 1.5254\n",
      "Epoch: 10/25... Step: 1290... Loss: 1.3920... Val Loss: 1.5317\n",
      "Epoch: 10/25... Step: 1300... Loss: 1.3888... Val Loss: 1.5267\n",
      "Epoch: 10/25... Step: 1310... Loss: 1.3838... Val Loss: 1.5220\n",
      "Epoch: 10/25... Step: 1320... Loss: 1.3536... Val Loss: 1.5212\n",
      "Epoch: 10/25... Step: 1330... Loss: 1.3537... Val Loss: 1.5185\n",
      "Epoch: 10/25... Step: 1340... Loss: 1.3452... Val Loss: 1.5179\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 10/25... Step: 1350... Loss: 1.3521... Val Loss: 1.5118\n",
      "Epoch: 10/25... Step: 1360... Loss: 1.3509... Val Loss: 1.5095\n",
      "Epoch: 10/25... Step: 1370... Loss: 1.3561... Val Loss: 1.5133\n",
      "Epoch: 10/25... Step: 1380... Loss: 1.3709... Val Loss: 1.5050\n",
      "Epoch: 10/25... Step: 1390... Loss: 1.3885... Val Loss: 1.5142\n",
      "Epoch: 11/25... Step: 1400... Loss: 1.3919... Val Loss: 1.5097\n",
      "Epoch: 11/25... Step: 1410... Loss: 1.4026... Val Loss: 1.4982\n",
      "Epoch: 11/25... Step: 1420... Loss: 1.3841... Val Loss: 1.5051\n",
      "Epoch: 11/25... Step: 1430... Loss: 1.3574... Val Loss: 1.5127\n",
      "Epoch: 11/25... Step: 1440... Loss: 1.3839... Val Loss: 1.5018\n",
      "Epoch: 11/25... Step: 1450... Loss: 1.3122... Val Loss: 1.4986\n",
      "Epoch: 11/25... Step: 1460... Loss: 1.3336... Val Loss: 1.4986\n",
      "Epoch: 11/25... Step: 1470... Loss: 1.3246... Val Loss: 1.4952\n",
      "Epoch: 11/25... Step: 1480... Loss: 1.3488... Val Loss: 1.4943\n",
      "Epoch: 11/25... Step: 1490... Loss: 1.3344... Val Loss: 1.4919\n",
      "Epoch: 11/25... Step: 1500... Loss: 1.3303... Val Loss: 1.4920\n",
      "Epoch: 11/25... Step: 1510... Loss: 1.3049... Val Loss: 1.4894\n",
      "Epoch: 11/25... Step: 1520... Loss: 1.3428... Val Loss: 1.4921\n",
      "Epoch: 12/25... Step: 1530... Loss: 1.3936... Val Loss: 1.4951\n",
      "Epoch: 12/25... Step: 1540... Loss: 1.3559... Val Loss: 1.4852\n",
      "Epoch: 12/25... Step: 1550... Loss: 1.3530... Val Loss: 1.4803\n",
      "Epoch: 12/25... Step: 1560... Loss: 1.3704... Val Loss: 1.4942\n",
      "Epoch: 12/25... Step: 1570... Loss: 1.3173... Val Loss: 1.4973\n",
      "Epoch: 12/25... Step: 1580... Loss: 1.2997... Val Loss: 1.4916\n",
      "Epoch: 12/25... Step: 1590... Loss: 1.2905... Val Loss: 1.4858\n",
      "Epoch: 12/25... Step: 1600... Loss: 1.3106... Val Loss: 1.4842\n",
      "Epoch: 12/25... Step: 1610... Loss: 1.3185... Val Loss: 1.4935\n",
      "Epoch: 12/25... Step: 1620... Loss: 1.3030... Val Loss: 1.4780\n",
      "Epoch: 12/25... Step: 1630... Loss: 1.3182... Val Loss: 1.4772\n",
      "Epoch: 12/25... Step: 1640... Loss: 1.2983... Val Loss: 1.4750\n",
      "Epoch: 12/25... Step: 1650... Loss: 1.2739... Val Loss: 1.4805\n",
      "Epoch: 12/25... Step: 1660... Loss: 1.3483... Val Loss: 1.4732\n",
      "Epoch: 13/25... Step: 1670... Loss: 1.3063... Val Loss: 1.4728\n",
      "Epoch: 13/25... Step: 1680... Loss: 1.3322... Val Loss: 1.4861\n",
      "Epoch: 13/25... Step: 1690... Loss: 1.2910... Val Loss: 1.4655\n",
      "Epoch: 13/25... Step: 1700... Loss: 1.2900... Val Loss: 1.4674\n",
      "Epoch: 13/25... Step: 1710... Loss: 1.2838... Val Loss: 1.4668\n",
      "Epoch: 13/25... Step: 1720... Loss: 1.2947... Val Loss: 1.4730\n",
      "Epoch: 13/25... Step: 1730... Loss: 1.3290... Val Loss: 1.4711\n",
      "Epoch: 13/25... Step: 1740... Loss: 1.2952... Val Loss: 1.4709\n",
      "Epoch: 13/25... Step: 1750... Loss: 1.2596... Val Loss: 1.4684\n",
      "Epoch: 13/25... Step: 1760... Loss: 1.2911... Val Loss: 1.4725\n",
      "Epoch: 13/25... Step: 1770... Loss: 1.3142... Val Loss: 1.4635\n",
      "Epoch: 13/25... Step: 1780... Loss: 1.2852... Val Loss: 1.4741\n",
      "Epoch: 13/25... Step: 1790... Loss: 1.2675... Val Loss: 1.4663\n",
      "Epoch: 13/25... Step: 1800... Loss: 1.2974... Val Loss: 1.4649\n",
      "Epoch: 14/25... Step: 1810... Loss: 1.3064... Val Loss: 1.4623\n",
      "Epoch: 14/25... Step: 1820... Loss: 1.2876... Val Loss: 1.4619\n",
      "Epoch: 14/25... Step: 1830... Loss: 1.3004... Val Loss: 1.4518\n",
      "Epoch: 14/25... Step: 1840... Loss: 1.2545... Val Loss: 1.4529\n",
      "Epoch: 14/25... Step: 1850... Loss: 1.2367... Val Loss: 1.4539\n",
      "Epoch: 14/25... Step: 1860... Loss: 1.2883... Val Loss: 1.4557\n",
      "Epoch: 14/25... Step: 1870... Loss: 1.3027... Val Loss: 1.4544\n",
      "Epoch: 14/25... Step: 1880... Loss: 1.2860... Val Loss: 1.4559\n",
      "Epoch: 14/25... Step: 1890... Loss: 1.2935... Val Loss: 1.4561\n",
      "Epoch: 14/25... Step: 1900... Loss: 1.2716... Val Loss: 1.4594\n",
      "Epoch: 14/25... Step: 1910... Loss: 1.2818... Val Loss: 1.4513\n",
      "Epoch: 14/25... Step: 1920... Loss: 1.2754... Val Loss: 1.4586\n",
      "Epoch: 14/25... Step: 1930... Loss: 1.2517... Val Loss: 1.4504\n",
      "Epoch: 14/25... Step: 1940... Loss: 1.3070... Val Loss: 1.4494\n",
      "Epoch: 15/25... Step: 1950... Loss: 1.2643... Val Loss: 1.4439\n",
      "Epoch: 15/25... Step: 1960... Loss: 1.2546... Val Loss: 1.4545\n",
      "Epoch: 15/25... Step: 1970... Loss: 1.2690... Val Loss: 1.4436\n",
      "Epoch: 15/25... Step: 1980... Loss: 1.2559... Val Loss: 1.4474\n",
      "Epoch: 15/25... Step: 1990... Loss: 1.2570... Val Loss: 1.4488\n",
      "Epoch: 15/25... Step: 2000... Loss: 1.2527... Val Loss: 1.4504\n",
      "Epoch: 15/25... Step: 2010... Loss: 1.2660... Val Loss: 1.4435\n",
      "Epoch: 15/25... Step: 2020... Loss: 1.2653... Val Loss: 1.4535\n",
      "Epoch: 15/25... Step: 2030... Loss: 1.2452... Val Loss: 1.4491\n",
      "Epoch: 15/25... Step: 2040... Loss: 1.2625... Val Loss: 1.4358\n",
      "Epoch: 15/25... Step: 2050... Loss: 1.2458... Val Loss: 1.4351\n",
      "Epoch: 15/25... Step: 2060... Loss: 1.2627... Val Loss: 1.4383\n",
      "Epoch: 15/25... Step: 2070... Loss: 1.2677... Val Loss: 1.4325\n",
      "Epoch: 15/25... Step: 2080... Loss: 1.2648... Val Loss: 1.4311\n",
      "Epoch: 16/25... Step: 2090... Loss: 1.2576... Val Loss: 1.4418\n",
      "Epoch: 16/25... Step: 2100... Loss: 1.2635... Val Loss: 1.4390\n",
      "Epoch: 16/25... Step: 2110... Loss: 1.2345... Val Loss: 1.4315\n",
      "Epoch: 16/25... Step: 2120... Loss: 1.2624... Val Loss: 1.4436\n",
      "Epoch: 16/25... Step: 2130... Loss: 1.2301... Val Loss: 1.4423\n",
      "Epoch: 16/25... Step: 2140... Loss: 1.2386... Val Loss: 1.4370\n",
      "Epoch: 16/25... Step: 2150... Loss: 1.2567... Val Loss: 1.4349\n",
      "Epoch: 16/25... Step: 2160... Loss: 1.2396... Val Loss: 1.4365\n",
      "Epoch: 16/25... Step: 2170... Loss: 1.2384... Val Loss: 1.4351\n",
      "Epoch: 16/25... Step: 2180... Loss: 1.2401... Val Loss: 1.4259\n",
      "Epoch: 16/25... Step: 2190... Loss: 1.2574... Val Loss: 1.4282\n",
      "Epoch: 16/25... Step: 2200... Loss: 1.2334... Val Loss: 1.4302\n",
      "Epoch: 16/25... Step: 2210... Loss: 1.2012... Val Loss: 1.4271\n",
      "Epoch: 16/25... Step: 2220... Loss: 1.2459... Val Loss: 1.4305\n",
      "Epoch: 17/25... Step: 2230... Loss: 1.2327... Val Loss: 1.4317\n",
      "Epoch: 17/25... Step: 2240... Loss: 1.2207... Val Loss: 1.4336\n",
      "Epoch: 17/25... Step: 2250... Loss: 1.2193... Val Loss: 1.4386\n",
      "Epoch: 17/25... Step: 2260... Loss: 1.2209... Val Loss: 1.4294\n",
      "Epoch: 17/25... Step: 2270... Loss: 1.2340... Val Loss: 1.4258\n",
      "Epoch: 17/25... Step: 2280... Loss: 1.2355... Val Loss: 1.4221\n",
      "Epoch: 17/25... Step: 2290... Loss: 1.2438... Val Loss: 1.4208\n",
      "Epoch: 17/25... Step: 2300... Loss: 1.2003... Val Loss: 1.4219\n",
      "Epoch: 17/25... Step: 2310... Loss: 1.2288... Val Loss: 1.4282\n",
      "Epoch: 17/25... Step: 2320... Loss: 1.2139... Val Loss: 1.4209\n",
      "Epoch: 17/25... Step: 2330... Loss: 1.2047... Val Loss: 1.4180\n",
      "Epoch: 17/25... Step: 2340... Loss: 1.2357... Val Loss: 1.4196\n",
      "Epoch: 17/25... Step: 2350... Loss: 1.2337... Val Loss: 1.4149\n",
      "Epoch: 17/25... Step: 2360... Loss: 1.2386... Val Loss: 1.4174\n",
      "Epoch: 18/25... Step: 2370... Loss: 1.2063... Val Loss: 1.4203\n",
      "Epoch: 18/25... Step: 2380... Loss: 1.2075... Val Loss: 1.4187\n",
      "Epoch: 18/25... Step: 2390... Loss: 1.2226... Val Loss: 1.4180\n",
      "Epoch: 18/25... Step: 2400... Loss: 1.2380... Val Loss: 1.4215\n",
      "Epoch: 18/25... Step: 2410... Loss: 1.2291... Val Loss: 1.4135\n",
      "Epoch: 18/25... Step: 2420... Loss: 1.2146... Val Loss: 1.4124\n",
      "Epoch: 18/25... Step: 2430... Loss: 1.2194... Val Loss: 1.4091\n",
      "Epoch: 18/25... Step: 2440... Loss: 1.2096... Val Loss: 1.4132\n",
      "Epoch: 18/25... Step: 2450... Loss: 1.1985... Val Loss: 1.4137\n",
      "Epoch: 18/25... Step: 2460... Loss: 1.2158... Val Loss: 1.4148\n",
      "Epoch: 18/25... Step: 2470... Loss: 1.2143... Val Loss: 1.4112\n",
      "Epoch: 18/25... Step: 2480... Loss: 1.1941... Val Loss: 1.4065\n",
      "Epoch: 18/25... Step: 2490... Loss: 1.2042... Val Loss: 1.4101\n",
      "Epoch: 18/25... Step: 2500... Loss: 1.1936... Val Loss: 1.4180\n",
      "Epoch: 19/25... Step: 2510... Loss: 1.2009... Val Loss: 1.4078\n",
      "Epoch: 19/25... Step: 2520... Loss: 1.2256... Val Loss: 1.4061\n",
      "Epoch: 19/25... Step: 2530... Loss: 1.2262... Val Loss: 1.4131\n",
      "Epoch: 19/25... Step: 2540... Loss: 1.2273... Val Loss: 1.4128\n",
      "Epoch: 19/25... Step: 2550... Loss: 1.1917... Val Loss: 1.4051\n",
      "Epoch: 19/25... Step: 2560... Loss: 1.2114... Val Loss: 1.4020\n",
      "Epoch: 19/25... Step: 2570... Loss: 1.1892... Val Loss: 1.4046\n",
      "Epoch: 19/25... Step: 2580... Loss: 1.2257... Val Loss: 1.4099\n",
      "Epoch: 19/25... Step: 2590... Loss: 1.1856... Val Loss: 1.4126\n",
      "Epoch: 19/25... Step: 2600... Loss: 1.1860... Val Loss: 1.4042\n",
      "Epoch: 19/25... Step: 2610... Loss: 1.1933... Val Loss: 1.4053\n",
      "Epoch: 19/25... Step: 2620... Loss: 1.1775... Val Loss: 1.4124\n",
      "Epoch: 19/25... Step: 2630... Loss: 1.1747... Val Loss: 1.4004\n",
      "Epoch: 19/25... Step: 2640... Loss: 1.1988... Val Loss: 1.4035\n",
      "Epoch: 20/25... Step: 2650... Loss: 1.2040... Val Loss: 1.4005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 20/25... Step: 2660... Loss: 1.2075... Val Loss: 1.4052\n",
      "Epoch: 20/25... Step: 2670... Loss: 1.2201... Val Loss: 1.3996\n",
      "Epoch: 20/25... Step: 2680... Loss: 1.1977... Val Loss: 1.4032\n",
      "Epoch: 20/25... Step: 2690... Loss: 1.1861... Val Loss: 1.4024\n",
      "Epoch: 20/25... Step: 2700... Loss: 1.2032... Val Loss: 1.3985\n",
      "Epoch: 20/25... Step: 2710... Loss: 1.1679... Val Loss: 1.3981\n",
      "Epoch: 20/25... Step: 2720... Loss: 1.1654... Val Loss: 1.4067\n",
      "Epoch: 20/25... Step: 2730... Loss: 1.1676... Val Loss: 1.3982\n",
      "Epoch: 20/25... Step: 2740... Loss: 1.1685... Val Loss: 1.4027\n",
      "Epoch: 20/25... Step: 2750... Loss: 1.1802... Val Loss: 1.3997\n",
      "Epoch: 20/25... Step: 2760... Loss: 1.1648... Val Loss: 1.4061\n",
      "Epoch: 20/25... Step: 2770... Loss: 1.2017... Val Loss: 1.3996\n",
      "Epoch: 20/25... Step: 2780... Loss: 1.2382... Val Loss: 1.3947\n",
      "Epoch: 21/25... Step: 2790... Loss: 1.2119... Val Loss: 1.3967\n",
      "Epoch: 21/25... Step: 2800... Loss: 1.2232... Val Loss: 1.3970\n",
      "Epoch: 21/25... Step: 2810... Loss: 1.2166... Val Loss: 1.3963\n",
      "Epoch: 21/25... Step: 2820... Loss: 1.1895... Val Loss: 1.4010\n",
      "Epoch: 21/25... Step: 2830... Loss: 1.2027... Val Loss: 1.3975\n",
      "Epoch: 21/25... Step: 2840... Loss: 1.1459... Val Loss: 1.3989\n",
      "Epoch: 21/25... Step: 2850... Loss: 1.1690... Val Loss: 1.3935\n",
      "Epoch: 21/25... Step: 2860... Loss: 1.1526... Val Loss: 1.3924\n",
      "Epoch: 21/25... Step: 2870... Loss: 1.1750... Val Loss: 1.3961\n",
      "Epoch: 21/25... Step: 2880... Loss: 1.1680... Val Loss: 1.3955\n",
      "Epoch: 21/25... Step: 2890... Loss: 1.1716... Val Loss: 1.3937\n",
      "Epoch: 21/25... Step: 2900... Loss: 1.1485... Val Loss: 1.3967\n",
      "Epoch: 21/25... Step: 2910... Loss: 1.1766... Val Loss: 1.3952\n",
      "Epoch: 22/25... Step: 2920... Loss: 1.2886... Val Loss: 1.3910\n",
      "Epoch: 22/25... Step: 2930... Loss: 1.1938... Val Loss: 1.3876\n",
      "Epoch: 22/25... Step: 2940... Loss: 1.1978... Val Loss: 1.3892\n",
      "Epoch: 22/25... Step: 2950... Loss: 1.2085... Val Loss: 1.3938\n",
      "Epoch: 22/25... Step: 2960... Loss: 1.1591... Val Loss: 1.3904\n",
      "Epoch: 22/25... Step: 2970... Loss: 1.1379... Val Loss: 1.3909\n",
      "Epoch: 22/25... Step: 2980... Loss: 1.1453... Val Loss: 1.3925\n",
      "Epoch: 22/25... Step: 2990... Loss: 1.1544... Val Loss: 1.3871\n",
      "Epoch: 22/25... Step: 3000... Loss: 1.1506... Val Loss: 1.3936\n",
      "Epoch: 22/25... Step: 3010... Loss: 1.1516... Val Loss: 1.3993\n",
      "Epoch: 22/25... Step: 3020... Loss: 1.1815... Val Loss: 1.3957\n",
      "Epoch: 22/25... Step: 3030... Loss: 1.1470... Val Loss: 1.3908\n",
      "Epoch: 22/25... Step: 3040... Loss: 1.1322... Val Loss: 1.3928\n",
      "Epoch: 22/25... Step: 3050... Loss: 1.1889... Val Loss: 1.3879\n",
      "Epoch: 23/25... Step: 3060... Loss: 1.1744... Val Loss: 1.3866\n",
      "Epoch: 23/25... Step: 3070... Loss: 1.1768... Val Loss: 1.3846\n",
      "Epoch: 23/25... Step: 3080... Loss: 1.1466... Val Loss: 1.3831\n",
      "Epoch: 23/25... Step: 3090... Loss: 1.1496... Val Loss: 1.3863\n",
      "Epoch: 23/25... Step: 3100... Loss: 1.1398... Val Loss: 1.3862\n",
      "Epoch: 23/25... Step: 3110... Loss: 1.1400... Val Loss: 1.3900\n",
      "Epoch: 23/25... Step: 3120... Loss: 1.1705... Val Loss: 1.3876\n",
      "Epoch: 23/25... Step: 3130... Loss: 1.1550... Val Loss: 1.3806\n",
      "Epoch: 23/25... Step: 3140... Loss: 1.1154... Val Loss: 1.3883\n",
      "Epoch: 23/25... Step: 3150... Loss: 1.1559... Val Loss: 1.3952\n",
      "Epoch: 23/25... Step: 3160... Loss: 1.1713... Val Loss: 1.3999\n",
      "Epoch: 23/25... Step: 3170... Loss: 1.1414... Val Loss: 1.3900\n",
      "Epoch: 23/25... Step: 3180... Loss: 1.1256... Val Loss: 1.3940\n",
      "Epoch: 23/25... Step: 3190... Loss: 1.1553... Val Loss: 1.3845\n",
      "Epoch: 24/25... Step: 3200... Loss: 1.1729... Val Loss: 1.3819\n",
      "Epoch: 24/25... Step: 3210... Loss: 1.1452... Val Loss: 1.3801\n",
      "Epoch: 24/25... Step: 3220... Loss: 1.1700... Val Loss: 1.3853\n",
      "Epoch: 24/25... Step: 3230... Loss: 1.1299... Val Loss: 1.3831\n",
      "Epoch: 24/25... Step: 3240... Loss: 1.1072... Val Loss: 1.3861\n",
      "Epoch: 24/25... Step: 3250... Loss: 1.1523... Val Loss: 1.3970\n",
      "Epoch: 24/25... Step: 3260... Loss: 1.1659... Val Loss: 1.3841\n",
      "Epoch: 24/25... Step: 3270... Loss: 1.1605... Val Loss: 1.3866\n",
      "Epoch: 24/25... Step: 3280... Loss: 1.1590... Val Loss: 1.3876\n",
      "Epoch: 24/25... Step: 3290... Loss: 1.1435... Val Loss: 1.3871\n",
      "Epoch: 24/25... Step: 3300... Loss: 1.1475... Val Loss: 1.4017\n",
      "Epoch: 24/25... Step: 3310... Loss: 1.1518... Val Loss: 1.3932\n",
      "Epoch: 24/25... Step: 3320... Loss: 1.1196... Val Loss: 1.3924\n",
      "Epoch: 24/25... Step: 3330... Loss: 1.1636... Val Loss: 1.3820\n",
      "Epoch: 25/25... Step: 3340... Loss: 1.1455... Val Loss: 1.3864\n",
      "Epoch: 25/25... Step: 3350... Loss: 1.1533... Val Loss: 1.3826\n",
      "Epoch: 25/25... Step: 3360... Loss: 1.1449... Val Loss: 1.3786\n",
      "Epoch: 25/25... Step: 3370... Loss: 1.1376... Val Loss: 1.3874\n",
      "Epoch: 25/25... Step: 3380... Loss: 1.1359... Val Loss: 1.3876\n",
      "Epoch: 25/25... Step: 3390... Loss: 1.1235... Val Loss: 1.3933\n",
      "Epoch: 25/25... Step: 3400... Loss: 1.1432... Val Loss: 1.3828\n",
      "Epoch: 25/25... Step: 3410... Loss: 1.1479... Val Loss: 1.3853\n",
      "Epoch: 25/25... Step: 3420... Loss: 1.1320... Val Loss: 1.3886\n",
      "Epoch: 25/25... Step: 3430... Loss: 1.1420... Val Loss: 1.3869\n",
      "Epoch: 25/25... Step: 3440... Loss: 1.1242... Val Loss: 1.3894\n",
      "Epoch: 25/25... Step: 3450... Loss: 1.1240... Val Loss: 1.3925\n",
      "Epoch: 25/25... Step: 3460... Loss: 1.1368... Val Loss: 1.3921\n",
      "Epoch: 25/25... Step: 3470... Loss: 1.1410... Val Loss: 1.3812\n"
     ]
    }
   ],
   "source": [
    "n_seqs, n_steps = 128, 100\n",
    "train(net, encoded, epochs=25, n_seqs=n_seqs, n_steps=n_steps, lr=0.001, cuda=True, print_every=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = {'n_hidden': net.n_hidden,\n",
    "              'n_layers': net.n_layers,\n",
    "              'state_dict': net.state_dict(),\n",
    "              'tokens': net.chars}\n",
    "with open('rnn.net', 'wb') as f:\n",
    "    torch.save(checkpoint, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('rnn.net', 'rb') as f:\n",
    "    checkpoint = torch.load(f)\n",
    "    \n",
    "loaded = CharRNN(checkpoint['tokens'], n_hidden=checkpoint['n_hidden'], n_layers=checkpoint['n_layers'])\n",
    "loaded.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(net, size, prime='The', top_k=None, cuda=False):\n",
    "        \n",
    "    if cuda:\n",
    "        net.cuda()\n",
    "    else:\n",
    "        net.cpu()\n",
    "\n",
    "    net.eval()\n",
    "    \n",
    "    # First off, run through the prime characters\n",
    "    chars = [ch for ch in prime]\n",
    "    h = net.init_hidden(1)\n",
    "    for ch in prime:\n",
    "        char, h = net.predict(ch, h, cuda=cuda, top_k=top_k)\n",
    "\n",
    "    chars.append(char)\n",
    "    \n",
    "    # Now pass in the previous character and get a new one\n",
    "    for ii in range(size):\n",
    "        char, h = net.predict(chars[-1], h, cuda=cuda, top_k=top_k)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\users\\swapnil.walke\\appdata\\local\\continuum\\anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:49: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "c:\\users\\swapnil.walke\\appdata\\local\\continuum\\anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:53: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "c:\\users\\swapnil.walke\\appdata\\local\\continuum\\anaconda3\\envs\\pytorch0.4\\lib\\site-packages\\ipykernel_launcher.py:56: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anna, that the\n",
      "mirst were completely turned with thotger, and at the beauty of hos\n",
      "its wearted and sancage and collacted with all the mother with an old painter\n",
      "of words, some art, a smint.\n",
      "\n",
      "\"I've not thought to be in the subject,\" said the old man,\n",
      "and still made the conversation, but his secretary and talking of\n",
      "his fingers, he still. He sat down in the did not care to see her attitude\n",
      "on the first room.\n",
      "\n",
      "Sht liked the sick smart still seemed in his father when he had\n",
      "to go, sure the shoulders to bring a changing soft of case, and would not have\n",
      "seemed to give if he would stop his house.\n",
      "\n",
      "And was all the part to her them. The first man all went in, and\n",
      "he seemed that the more said that it was the footman so important.\n",
      "She had so disapproved of them, and had not thrusted on the sease,\n",
      "as that she had never saying a perceasable position in her\n",
      "face and a little work, but to go to his betray, but there was\n",
      "not time to disleap in his counting house. Thene where when he was\n",
      "not stupid, and trues that the same secret, and was still at\n",
      "an impossible of continual to both a musical or the same all\n",
      "the previousness who had thinks and done which there were been framing his\n",
      "owide and attacked on timidly a mercheat of an election of the call and\n",
      "desponded the man a place of an important, though he did not know that the\n",
      "words that was sure to show her heart as though he had been to great\n",
      "three memory and strength the pain with which she saw, who had struggled his\n",
      "hand and he supposed the first three troubling of all her sense, what\n",
      "seeing her, and, when she was attaching the stables with his\n",
      "bark, with with strenges flowers to betsee her attention of\n",
      "heart, that he had been to be through and the side. A consequence. The\n",
      "mild wanter of a second strange her sincerest whether the conversation was\n",
      "the selves of supporting on his father's head, but he was at the same of\n",
      "the cruminess with she was as he went into his hand.\n",
      "\n",
      "\"I shall be seeing that to be stepped out of all the possible. Th\n"
     ]
    }
   ],
   "source": [
    "print(sample(net, 2000, prime='Anna', top_k=5, cuda=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
